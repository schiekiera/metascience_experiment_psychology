# Metascience experiment psychology
This repository contains anonymized data for a metascientific experiment on the influence of publication bias on psychologists' assessments of clinical psychology abstracts. The scripts are prepared so that the data will be directly loaded from the GitHub repository link without downloading the data manually. All main hypotheses are fully reproducible. Parts of the data for the exploratory analysis are anonymized because they contain sensitive information (e.g., age, gender, conscientiousness).

## Abstract
### Background: 
Publishing all study results, including non-significant and hypothesis-inconsistent findings, is central to the process of scientific knowledge production. Yet, review studies suggest that results that are statistically significant or consistent with hypotheses are preferred in the publication process and in reception. However, questions remain about the mechanisms underlying publication bias, and work has focused on between-subjects designs rather than within-subjects experiments. Using an experimental *within-subject* design, we investigated decision-making processes based on *Dual Process Theories* to understand selective (non-)publication and (non-)reception. Specifically, we examined (1) *intuitive* and (2) *considered* evaluations of research abstracts, along with the accompanying (3) *Feeling of Rightness* (FOR) of intuitive evaluations, as functions of the experimental variation of statistical significance and hypothesis-consistency. 
### Methods:
In each of four online experiments (each n = 75), 16 fictitious research paper abstracts were randomly presented to a sample of clinical psychology researchers. Participants were first asked for the likelihood of submitting the paper for publication, reading the paper, or citing the paper based solely on the abstract, providing a fast and *intuitive* evaluation. They were then asked to rate the certainty of this response (FOR), and finally, they were prompted to rethink the decision and provide a more *considered* evaluation. For each experiment we fitted *multilevel* and *multilevel mediation* models to investigate whether (1) intuitive evaluations are associated with the treatment variable or (2a) whether response changes between considered and intuitive evaluations are mediated by FOR and (2b) directly associated with the treatment variable.
### Results:
Researchers rated statistically non-significant abstracts as less likely to be submitted, read, or cited compared to significant ones. No such bias was found for hypothesis-inconsistent results. In most cases, initial intuitive evaluations were not substantially revised after deliberation. However, hypothesis-inconsistent results slightly increased reading likelihood upon reconsideration, and FOR mediated the effect of statistical significance on submission decisions—unexpectedly amplifying, rather than correcting, the bias.
### Conclusion:
The findings suggest a persistent bias against statistically non-significant results in researchers’ publication, citation and reception decisions, while hypothesis-consistency does not seem to affect the decisions. Contrary to Dual Process Theory predictions, deliberation did not reduce bias, and uncertainty (low FOR) was mostly not associated with observed likelihoods upon reconsideration regarding publication, citation and reception. Together, these findings point to persistent publication and reception biases with respect to significant results at the level of individual researchers. This affects cumulative knowledge building and the reproducibility of research. 
